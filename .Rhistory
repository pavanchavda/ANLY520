# Pick the first suggestion
words_r.sugg <- unlist(lapply(words_r.sugg, function(x) x[1]))
words_r.dict <- as.data.frame(cbind(words_r.errors,words_r.sugg))
words_r.dict$words_r.pattern <- paste0("\\b", words_r.dict$words_r.errors, "\\b")
#Replace the words
stri_replace_all_regex(str = wordlist,
pattern = words_r.dict$words_r.pattern,
replacement = words_r.dict$words_r.sugg,
vectorize_all = FALSE)
# Spell check the words
words_r.errors <- hunspell(words_r_rpcont)
words_r.sugg <- hunspell_suggest(unlist(words_r.errors), dict = dictionary("en_US"))
# Pick the first suggestion
words_r.sugg <- unlist(lapply(words_r.sugg, function(x) x[1]))
words_r.dict <- as.data.frame(cbind(words_r.errors,words_r.sugg))
words_r.dict$words_r.pattern <- paste0("\\b", words_r.dict$words_r.errors, "\\b")
#Replace the words
stri_replace_all_regex(str = words_r_rpcont,
pattern = words_r.dict$words_r.pattern,
replacement = words_r.dict$words_r.sugg,
vectorize_all = FALSE)
words_r.sugg
# Spell check the words
words_r.errors <- hunspell(words_r_rpcont)
words_r.sugg <- hunspell_suggest(unlist(words_r.errors), dict = dictionary("en_US"))
# Pick the first suggestion
words_r.sugg <- unlist(lapply(words_r.sugg, function(x) x[1]))
words_r.dict <- as.data.frame(cbind(words_r.errors,words_r.sugg))
words_r.dict$words_r.pattern <- paste0("\\b", words_r.dict$words_r.errors, "\\b")
#Replace the words
words_r.splchk <- stri_replace_all_regex(str = words_r_rpcont,
pattern = words_r.dict$words_r.pattern,
replacement = words_r.dict$words_r.sugg,
vectorize_all = FALSE)
words_r.lemd <- lematize_words(words_r.splchk)
words_r.lemd <- lemmatize_words(words_r.splchk)
##r chunk
words_r.rmstp <- removeWords(words_r.lemd, stopwords(kind = "SMART"))
##r chunk
words_r.tokd <- tokenize_words(words_r.rmstp,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
##r chunk
words_r.rmstp <- removeWords(words_r.lemd, stopwords(kind = "SMART"))
words_r.tokd <- tokenize_words(words_r.rmstp,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
print(words_r.tokd[1:100])
library(textclean);library(tokenizers)
words_r = unlist(tokenize_words(hca_rmsymb))
words_r = str_replace_all(words_r, pattern = "’",
replacement = "'")
words_r[49]
# Replacing contractions
words_r_rpcont <- replace_contraction(words_r,
contraction.key = lexicon::key_contractions, #default
ignore.case = T)
print(contractions_dict)
#do not change this
knitr::opts_chunk$set(echo = TRUE)
library(reticulate); library(Rling); library("rvest"); library("stringr"); library("stringi");
library("textclean");library("hunspell"); library("tm"); library("textstem")
# Importing blog page
hca <- read_html("https://healthinformatics.uic.edu/blog/how-health-care-analytics-improves-patient-care/")
# Removing html tags
clean_hca <- html_text(hca)
clean_hca <- tolower(clean_hca)
hca_rmsymb <- stri_trans_general(str = clean_hca, id = "Latin-ASCII")
library(textclean);library(tokenizers)
words_r = unlist(tokenize_words(hca_rmsymb))
words_r = str_replace_all(words_r, pattern = "'",
replacement = "'")
words_r[49]
# Replacing contractions
words_r_rpcont <- replace_contraction(words_r,
contraction.key = lexicon::key_contractions, #default
ignore.case = T)
# Spell check the words
words_r.errors <- hunspell(words_r_rpcont)
words_r.sugg <- hunspell_suggest(unlist(words_r.errors), dict = dictionary("en_US"))
# Pick the first suggestion
words_r.sugg <- unlist(lapply(words_r.sugg, function(x) x[1]))
words_r.dict <- as.data.frame(cbind(words_r.errors,words_r.sugg))
words_r.dict$words_r.pattern <- paste0("\\b", words_r.dict$words_r.errors, "\\b")
#Replace the words
words_r.splchk <- stri_replace_all_regex(str = words_r_rpcont,
pattern = words_r.dict$words_r.pattern,
replacement = words_r.dict$words_r.sugg,
vectorize_all = FALSE)
library(textclean);library(tokenizers)
words_r = unlist(tokenize_words(hca_rmsymb))
words_r = str_replace_all(words_r, pattern = "'",
replacement = "'")
words_r[49]
# Replacing contractions
words_r <- replace_contraction(words_r,
contraction.key = lexicon::key_contractions, #default
ignore.case = T)
# Spell check the words
words_r.errors <- hunspell(words_r)
words_r.sugg <- hunspell_suggest(unlist(words_r.errors), dict = dictionary("en_US"))
# Pick the first suggestion
words_r.sugg <- unlist(lapply(words_r.sugg, function(x) x[1]))
words_r.dict <- as.data.frame(cbind(words_r.errors,words_r.sugg))
words_r.dict$words_r.pattern <- paste0("\\b", words_r.dict$words_r.errors, "\\b")
#Replace the words
words_r.splchk <- stri_replace_all_regex(str = words_r_rpcont,
pattern = words_r.dict$words_r.pattern,
replacement = words_r.dict$words_r.sugg,
vectorize_all = FALSE)
# Spell check the words
words_r.errors <- hunspell(words_r)
words_r.sugg <- hunspell_suggest(unlist(words_r.errors), dict = dictionary("en_US"))
# Pick the first suggestion
words_r.sugg <- unlist(lapply(words_r.sugg, function(x) x[1]))
words_r.dict <- as.data.frame(cbind(words_r.errors,words_r.sugg))
words_r.dict$words_r.pattern <- paste0("\\b", words_r.dict$words_r.errors, "\\b")
#Replace the words
words_r.splchk <- stri_replace_all_regex(str = words_r,
pattern = words_r.dict$words_r.pattern,
replacement = words_r.dict$words_r.sugg,
vectorize_all = FALSE)
words_r.dict
words_r.errors
words_r.sugg
words_r.dict$words_r.pattern
words_r.splchk
words_r.lemd <- lemmatize_words(words_r.splchk)
py_available(spacy)
py_install("Spacy", pip = T)
repl_python()
words_py
type(words_py)
quit
repl_python()
type(words_py)
repl_python()
type(words_py)
repl_python()
type(words_py)
quit
##r chunk
words_r.rmstp <- removeWords(words_r.lemd, stopwords(kind = "SMART"))
repl_python()
nltk.download(“stopwords”)
nltk.download('stopwords')
quit
repl_python()
nltk.download('punkt')
words_r.tokd <- tokenize_words(words_r.rmstp,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
repl_python()
type(words_py)
quit
print(words_r.tokd[1:100])
repl_python()
print(words_py_splchk[1:100])
repl_python()
print(words_py[1:100])
quit
repl_python()
textblob(words_py)
quit
##r chunk
words_r.rmstp <- removeWords(words_r.lemd, stopwords(kind = "SMART"))
words_r.tokd <- tokenize_words(words_r.rmstp,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
repl_python()
type(words_py_rmstp)
print(words_py_rmstp[:100])
print(words_py_lemd[:100])
print(words_py_str[:100])
print(words_py_splchk[:100])
print(blob[:100])
#do not change this
knitr::opts_chunk$set(echo = TRUE)
library(reticulate); library(Rling); library("rvest"); library("stringr"); library("stringi");
library("textclean");library("hunspell"); library("tm"); library("textstem")
# Importing blog page
clean_text_r <- read_html("https://www.aggieerin.com/post/line-endings-are-dumb/")
# Removing html tags
clean_text_r <- html_text(clean_text_r)
clean_text_r <- tolower(clean_text_r)
substr(clean_text_r, 1000,2000)
clean_text_r <- stri_trans_general(str = clean_text_r, id = "Latin-ASCII")
substr(clean_text_r, 1000,2000)
clean_text_r <- stri_trans_general(str = clean_text_r, id = "Latin-ASCII")
substr(clean_text_r, 1000,2000)
library(textclean);library(tokenizers)
clean_text_r = unlist(tokenize_words(clean_text_r))
clean_text_r = str_replace_all(clean_text_r, pattern = "'",
replacement = "'")
# Replacing contractions
clean_text_r <- replace_contraction(clean_text_r,
contraction.key = lexicon::key_contractions, #default
ignore.case = T)
substr(clean_text_r, 1000,2000)
library(textclean);library(tokenizers)
clean_text_r = unlist(tokenize_words(clean_text_r))
clean_text_r = str_replace_all(clean_text_r, pattern = "'",
replacement = "'")
# Replacing contractions
clean_text_r <- replace_contraction(clean_text_r,
contraction.key = lexicon::key_contractions, #default
ignore.case = T)[50]
#substr(clean_text_r, 1000,2000)
# Importing blog page
clean_text_r <- read_html("https://www.aggieerin.com/post/line-endings-are-dumb/")
# Removing html tags
clean_text_r <- html_text(clean_text_r)
substr(clean_text_r, 1000,2000)
clean_text_r <- tolower(clean_text_r)
substr(clean_text_r, 1000,2000)
clean_text_r <- stri_trans_general(str = clean_text_r, id = "Latin-ASCII")
substr(clean_text_r, 1000,2000)
library(textclean);library(tokenizers)
clean_text_r = unlist(tokenize_words(clean_text_r))
clean_text_r = str_replace_all(clean_text_r, pattern = "'",
replacement = "'")
# Replacing contractions
clean_text_r <- replace_contraction(clean_text_r,
contraction.key = lexicon::key_contractions, #default
ignore.case = T)
substr(clean_text_r, 1,50)
clean_text_r[50]
# Spell check the words
spelling.errors <- hunspell(clean_text_r)
spelling.sugg <- hunspell_suggest(unlist(spelling.errors), dict = dictionary("en_US"))
# Pick the first suggestion
spelling.sugg <- unlist(lapply(spelling.sugg, function(x) x[1]))
spelling.dict <- as.data.frame(cbind(spelling.errors,spelling.sugg))
spelling.dict$spelling.pattern <- paste0("\\b", spelling.dict$spelling.errors, "\\b")
#Replace the words
clean_text_r <- stri_replace_all_regex(str = clean_text_r,
pattern = spelling.dict$spelling.pattern,
replacement = spelling.dict$spelling.sugg,
vectorize_all = FALSE)
clean_text_r[50]
repl_python()
type(clean_text_py)
word(clean_text_py)
word_py[1000:2000]
type(word_py)
type(corrected_py)
type(clean_text_py)
quit
repl_python()
type(clean_text_py)
type(corrected_py)
quit
repl_python()
corrected_py[1000:1005]
words_py[1000:1005]
word_py[1000:1005]
repl_python()
clean_text_py[1000:1005]
clean_text_py[1000:1010]
word_py[1000:1010]
corrected_py[1000:1010]
quit
clean_text_r <- lemmatize_words(clean_text_r)
substr(clean_text_r, 1000, 2000)
##r chunk
clean_text_r <- removeWords(clean_text_r, stopwords(kind = "SMART"))
clean_text_r[50:100]
words_r.tokd <- tokenize_words(words_r.rmstp,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
clean_text_r <- tokenize_words(words_r.rmstp,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
clean_text_r <- tokenize_words(clean_text_r,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
clean_text_r[50:100]
repl_python()
type(clean_text_py)
quit
repl_python()
type(clean_text_py)
quit
repl_python()
type(clean_text_py)
quit
clean_text_r <- tokenize_words(clean_text_r,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
# Importing blog page
clean_text_r <- read_html("https://www.aggieerin.com/post/line-endings-are-dumb/")
# Removing html tags
clean_text_r <- html_text(clean_text_r)
substr(clean_text_r, 1000,2000)
clean_text_r <- tolower(clean_text_r)
substr(clean_text_r, 1000,2000)
clean_text_r <- stri_trans_general(str = clean_text_r, id = "Latin-ASCII")
substr(clean_text_r, 1000,2000)
library(textclean);library(tokenizers)
clean_text_r = unlist(tokenize_words(clean_text_r))
clean_text_r = str_replace_all(clean_text_r, pattern = "'",
replacement = "'")
# Replacing contractions
clean_text_r <- replace_contraction(clean_text_r,
contraction.key = lexicon::key_contractions, #default
ignore.case = T)
clean_text_r[50:60]
# Spell check the words
spelling.errors <- hunspell(clean_text_r)
spelling.sugg <- hunspell_suggest(unlist(spelling.errors), dict = dictionary("en_US"))
# Pick the first suggestion
spelling.sugg <- unlist(lapply(spelling.sugg, function(x) x[1]))
spelling.dict <- as.data.frame(cbind(spelling.errors,spelling.sugg))
spelling.dict$spelling.pattern <- paste0("\\b", spelling.dict$spelling.errors, "\\b")
#Replace the words
clean_text_r <- stri_replace_all_regex(str = clean_text_r,
pattern = spelling.dict$spelling.pattern,
replacement = spelling.dict$spelling.sugg,
vectorize_all = FALSE)
clean_text_r[50:60]
clean_text_r <- lemmatize_words(clean_text_r)
clean_text_r[50:100]
##r chunk
clean_text_r <- removeWords(clean_text_r, stopwords(kind = "SMART"))
clean_text_r[50:100]
dim(clean_text_r)
str(clean_text_r)
clean_text_r <- tokenize_words(clean_text_r,
lowercase = T,
stopwords = NULL,
strip_punct = T,
strip_numeric = F,
simplify = F)
clean_text_r[50:60]
print(clean_text_r[0:100])
clean_text_r[0:100]
str(clean_text_r)
clean_text_r[0:100]
as.vector(clean_text_r[0:100])
as.vector(clean_text_r[0:100], "character")
str(clean_text_r)
repl_python()
type(clean_text_py)
library(reticulate)
py_config()
#do not change this
knitr::opts_chunk$set(echo = TRUE)
install.packages("Rserve")
library(Rserve)
knitr::opts_chunk$set(echo = TRUE)
##r chunk
library(reticulate); library(rvest); library(tokenizers); library(stringi); library(textclean); library(stringr); library(hunspell); library(textstem); library(stopwords); library(tm); library(wordnet); library("tagger"); library(dplyr); library(RDRPOSTagger); library(wordnet); library(readxl); library(dplyr)
#I'll leave these here for now
#data <- read_xlsx("lyrics_Pop.xlsx")
#data1 <- data %>% filter(index==1)
#lyrics <- data1$lyrics
repl_python()
quit
repl_python()
len(lyrics)
quit
#do not change this
knitr::opts_chunk$set(echo = TRUE)
##r chunk
library("reticulate"); library("rvest"); library("tokenizers"); library("stringi"); library("textclean"); library("stringr"); library("hunspell"); library("textstem"); library("stopwords"); library("tm"); library("wordnet"); library("tagger"); library("dplyr"); library("RDRPOSTagger"); library("wordnet"); library("readxl"); library("dplyr")
repl_python()
for tree in rd_parser.parse(sent2):
print(tree)
sent2
for tree in rd_parser.parse(sent2):print(tree)
for tree in rd_parser.parse(sent2):
print(tree)
for tree in rd_parser.parse(sent2):
print(tree)
quit
repl_python()
for tree2 in rd_parser.parse(sent2):
print(tree2)
quit
repl_python()
type(sent2)
rd_parser.parse(sent2)
rd_parser.parse(sent1)
rd_parser.parse(sent1).tree
quit
#do not change this
knitr::opts_chunk$set(echo = TRUE)
##r chunk
library("reticulate"); library("rvest"); library("tokenizers"); library("stringi"); library("textclean"); library("stringr"); library("hunspell"); library("textstem"); library("stopwords"); library("tm"); library("wordnet"); library("tagger"); library("dplyr"); library("RDRPOSTagger"); library("wordnet"); library("readxl"); library("dplyr")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
##r chunk
library("reticulate")
knitr::opts_chunk$set(echo = TRUE)
#Required Packages
library(caret)
library(readr)
#Load the Data
dataset <- read_csv("Absenteeism_at_work_train.csv")
str(dataset)
#prepare dataset
colnames(dataset) <- make.names(colnames(dataset))
#Load the Data
dataset <- read_csv("Absenteeism_at_work_train.csv")
str(dataset)
#prepare dataset
colnames(dataset) <- make.names(colnames(dataset))
#converting continuous target variable into categorical
group_0 <- dataset$Absenteeism.time.in.hours == 0
group_1 <- dataset$Absenteeism.time.in.hours > 0 & dataset$Absenteeism.time.in.hours <= 6
group_2 <- dataset$Absenteeism.time.in.hours > 6
dataset$Absenteeism.time.in.hours[group_0] <- 0
dataset$Absenteeism.time.in.hours[group_1] <- 1
dataset$Absenteeism.time.in.hours[group_2] <- 2
dataset$Absenteeism.time.in.hours = as.factor(dataset$Absenteeism.time.in.hours)
str(dataset)
summary(dataset)
#Create a list of 80% of the rows in the Original dataset we can use for training
validation_index = createDataPartition(dataset$Absenteeism.time.in.hours, p =0.80, list = FALSE)
dim(validation_index)
#Select 20% of the data for validation
Validation = dataset[-validation_index,]
dim(Validation)
#Use the remaining 80% of the data to training and testing the models
dataset = dataset[validation_index,]
dim(dataset)
#Summarize the Data
dim(dataset)
dim(Validation)
#check for NAs
apply(dataset,2,function(x) sum(is.na(x)))#Check for Missing...
apply(Validation,2,function(x) sum(is.na(x)))#Check for Missing...
#remove NAs
dataset <- na.omit(dataset)
Validation <- na.omit(Validation)
#Evaluate Algorithms for Modeling
#Test Harness
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
#Build some Models
# Classification and Reegression Trees (CART)
set.seed(7)
fit.cart <- train(Absenteeism.time.in.hours~., dataset, method="rpart", metric=metric, trControl=control)
# k-Nearest Neigbors (kNN)
set.seed(7)
fit.knn <- train(Absenteeism.time.in.hours~., dataset, method="knn", metric=metric, trControl=control)
# Support Vector Machines (SVM) with Linear Kernel
set.seed(7)
fit.svm <- train(Absenteeism.time.in.hours~., dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Absenteeism.time.in.hours~., dataset, method="rf", metric=metric, trControl=control)
#Select the Best Model
#Summarize the Results
results = resamples(list(cart = fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
#Visually compare the models accuracy
dotplot(results)
#Making Predictions
#Estimate Skill of CART in the Validation Set
predictions = predict(fit.cart, Validation)
confusionMatrix(predictions, Validation$Absenteeism.time.in.hours)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
##r chunk
library("reticulate")
repl_python()
predicted_sentiments.head()
quit
knitr::opts_chunk$set(echo = TRUE)
##r chunk
library("reticulate")
knitr::opts_chunk$set(echo = TRUE)
##r chunk
library("reticulate")
knitr::include_graphics("positive_topic1.png")
knitr::include_graphics("positive_topic1.png")
knitr::include_graphics("positive_topic2.png")
knitr::include_graphics("negative_topic1.png")
knitr::include_graphics("negative_topic2.png")
knitr::opts_chunk$set(echo = TRUE)
##r chunk
library("reticulate")
knitr::include_graphics("positive_topic1.png")
knitr::include_graphics("positive_topic2.png")
knitr::include_graphics("negative_topic1.png")
knitr::include_graphics("negative_topic2.png")
