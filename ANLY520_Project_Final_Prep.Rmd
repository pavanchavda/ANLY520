---
title: 'Executive 2: Final Preparation'
author: "Pavan Chavda, OluwaTobi A  kinyemi"
date: "Sys.Date()"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

This project should allow you to apply the information you've learned in the course to a new dataset. While the structure of the final project will be more of a research project, you can use this knowledge to appropriately answer questions in all fields, along with the practical skills of writing a report that others can read. The dataset must be related to language or language processing in some way. You must use an analysis we learned in class. 

This assign is preparation for the final project focused on text cleaning. You will find a dataset that matches what you are interested in for your final project (likely sentiment analysis, but entity recognition or another classification problem would be acceptable as well). You will import your dataset and clean the data using the steps listed below. You can change datasets between now and the final, but this project should get the code ready for the data cleaning section. 

### Method - Data - Variables

Explain the data you have selected to study. You can find data through many available corpora or other datasets online (ask for help here for sure!). How was the data collected? Who/what is in the data? 

### Clean the Data

You should include code to perform the following steps:

- Lower case
- Remove symbols
- Remove contractions
- Fix spelling errors
- Lemmatize the words
- Remove stopwords 

You can perform this analysis in Python or R. You will turn in a knitted file that shows the steps of the code, along with the final print out of the first few words for the finalized data. Be sure to save the data at each step and *do not* print it out until the end (you can make it print temporarily for yourself, but the final report should not be pages and pages of text printed out).

#### Lower case
```{r}
##r chunk
library(reticulate); library(reticulate); library(rvest); library(tokenizers); library(stringi); library(textclean); library(stringr); library(hunspell); library(textstem); library(stopwords); library(tm); library(wordnet); library("tagger"); library(dplyr); library(RDRPOSTagger); library(wordnet); library(readxl); library(dplyr)
py_config()
data <- read_xlsx("C:/Users/Pavan/Documents/GitHub/ANLY520/lyrics_Pop.xlsx")
data1 <- data %>% filter(index==1)
lyrics <- data1$lyrics
```


```{python}
##python chunk
import nltk
import pandas as pd
import spacy
import random
import unicodedata
import contractions
from bs4 import BeautifulSoup
from textblob import Word
import regex
import spacy
from nltk.corpus import stopwords
nlp = spacy.load('en_core_web_sm')

data = r.lyrics
data

data = data.lower()
```

#### Remove symbols
```{python }
def remove_accented_chars(text):
  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
  return text
  
data = remove_accented_chars(data)
data
```

#### Remove contractions
```{python }
from contractions import contractions_dict

for contraction, expansion in contractions_dict.items():
  data = data.replace(contraction, expansion)
  
data
```

#### Fix spelling errors
```{python }
words_py = nltk.word_tokenize(data)
corrected_py = [Word(word).correct() for word in words_py]
data = " ".join(corrected_py)

data
```

#### Lemmatize the words
```{python }
def lemmatize_text(text):
  text = nlp(text)
  text = " ".join([word.lemma_ if word.lemma_ != "-PRON-" else word.text for word in text])
  return text
  
data = lemmatize_text(data)
data
```

#### Remove stopwords
```{python }
set(stopwords.words('english'))
no_stop = [word for word in nltk.word_tokenize(data) if word not in stopwords.words('english')]
data = " ".join(no_stop)
data
```