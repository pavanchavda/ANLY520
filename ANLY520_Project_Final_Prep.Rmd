---
title: 'Executive 2: Final Preparation'
author: "Pavan Chavda, OluwaTobi Akinyemi"
date: "Sys.Date()"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

This project should allow you to apply the information you've learned in the course to a new dataset. While the structure of the final project will be more of a research project, you can use this knowledge to appropriately answer questions in all fields, along with the practical skills of writing a report that others can read. The dataset must be related to language or language processing in some way. You must use an analysis we learned in class. 

This assign is preparation for the final project focused on text cleaning. You will find a dataset that matches what you are interested in for your final project (likely sentiment analysis, but entity recognition or another classification problem would be acceptable as well). You will import your dataset and clean the data using the steps listed below. You can change datasets between now and the final, but this project should get the code ready for the data cleaning section. 

### Method - Data - Variables

Explain the data you have selected to study. You can find data through many available corpora or other datasets online (ask for help here for sure!). How was the data collected? Who/what is in the data? 

### Clean the Data

You should include code to perform the following steps:

- Lower case
- Remove symbols
- Remove contractions
- Fix spelling errors
- Lemmatize the words
- Remove stopwords 

You can perform this analysis in Python or R. You will turn in a knitted file that shows the steps of the code, along with the final print out of the first few words for the finalized data. Be sure to save the data at each step and *do not* print it out until the end (you can make it print temporarily for yourself, but the final report should not be pages and pages of text printed out).

Loading libraries
```{r warning=FALSE, message=FALSE }
##r chunk
library(reticulate); library(rvest); library(tokenizers); library(stringi); library(textclean); library(stringr); library(hunspell); library(textstem); library(stopwords); library(tm); library(wordnet); library("tagger"); library(dplyr); library(RDRPOSTagger); library(wordnet); library(readxl); library(dplyr)

#I'll leave these here for now
#data <- read_xlsx("lyrics_Pop.xlsx") 
#data1 <- data %>% filter(index==1)
#lyrics <- data1$lyrics
```

```{python }
##python chunk
import nltk
import pandas as pd
import spacy
import random
import unicodedata
import contractions
from bs4 import BeautifulSoup
from textblob import Word
import regex
import spacy
import xlrd
from nltk.corpus import stopwords
nlp = spacy.load('en_core_web_sm')

```

Importing and prep
```{python preparing data}
# Importing data
pop_songs = pd.read_excel(r'lyrics_Pop.xlsx') #this should work fine since the dataset is in the same folder with your soruce file
# Converting dataframe to list 
pop_songs = pop_songs.values.tolist()
# Shaping data into string
lyrics = []
# Loop over list and concatenate into single list
for row in pop_songs:
  song = str(row[5])
  lyrics.append(song)
# Concatenate all songs into one string
lyrics = ' '.join(lyrics)
# View first few lines 
lyrics[:200]
```

#### Remove contractions - Step 1

Running contractions one time before converting to all lower case seems to yield better results at expanding all pronoun based contractions such as `I'm` and `I'd`. It appears that once these are in lower case, the function does not readily recongnize the lower case `i` as a pronoun and flag it for expansion. Running the remove contractions step helps take care of this.

```{python }
from contractions import contractions_dict

for contraction, expansion in contractions_dict.items():
  lyrics = lyrics.replace(contraction, expansion)
  
lyrics[:1000]
```

#### Lower case


```{python lower case}

lyrics = lyrics.lower()
lyrics[:1000]
```

#### Remove symbols
```{python remove symbols}
def remove_accented_chars(text):
  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
  return text
  
lyrics = remove_accented_chars(lyrics)
lyrics[:1000]
```

We can see that we still have the `it's` contractions left in areas where a the wildcard for a new line `\n` was parsed into the data. Executing the remove contractions step again should take care of these.

#### Remove contractions - Step 2
```{python }
from contractions import contractions_dict

for contraction, expansion in contractions_dict.items():
  lyrics = lyrics.replace(contraction, expansion)
  
lyrics[:1000]
```


#### Fix spelling errors
```{python correct spelling}
lyrics = nltk.word_tokenize(lyrics[:10000])
lyrics_corrected = [Word(word).correct() for word in lyrics]
lyrics = " ".join(lyrics_corrected)

lyrics[:1000]
```

#### Lemmatize the words
```{python lemmatize}
def lemmatize_text(text):
  text = nlp(text)
  text = " ".join([word.lemma_ if word.lemma_ != "-PRON-" else word.text for word in text])
  return text
  
lyrics = lemmatize_text(lyrics)
lyrics[:1000]
```

#### Remove stopwords
```{python }
set(stopwords.words('english'))
no_stop = [word for word in nltk.word_tokenize(lyrics) if word not in stopwords.words('english')]
lyrics = " ".join(no_stop)
print('\n')
lyrics[:1000]
```