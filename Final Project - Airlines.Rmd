---
title: 'ANLY 520 Final Project - Sentiment Analysis of US Airline Industry Customers\' Tweets'
author: "Oluwatobi Akinyemi & Pavan Chavda"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the libraries + functions


- Load all the libraries or functions that you will use to for the rest of the assignment. It is helpful to define your libraries and functions at the top of a report, so that others can know what they need for the report to compile correctly. 
- Import the separate python file that includes the functions you will need for the classification reports. 

```{r libaries}
##r chunk
library("reticulate")
```

- Load the Python libraries or functions that you will use for that section. 

```{python}
##python chunk
import pandas as pd
import numpy as np
import nltk
import textblob
from bs4 import BeautifulSoup
import unicodedata
import contractions
#if you want to stem
from nltk import PorterStemmer
ps = PorterStemmer()
from model_evaluation_utils import display_model_performance_metrics
```

## The Data

- This dataset includes tweets that have been coded as either negative or positive. 
- Import the data using either R or Python. I put a Python chunk here because you will need one to import the data, but if you want to first import into R, that's fine. 

```{python}
##python chunk
dataset = pd.read_csv('tweets.csv')
dataset.shape
dataset.head()
print(dataset.info())
```
```{python data preprocessing}
#select desired columns
dataset = dataset[['airline','airline_sentiment','text']]
dataset.head()
```

## Clean up the data (text normalization)

- Use our clean text function from this lecture to clean the text for this analysis. 

```{python}
##python chunk
STOPWORDS = set(nltk.corpus.stopwords.words('english')) #stopwords
STOPWORDS.remove('no')
STOPWORDS.remove('but')
STOPWORDS.remove('not')

def clean_text(text):
    text = BeautifulSoup(text).get_text() #html
    text = text.lower() #lower case
    text = contractions.fix(text) #contractions
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') #symbols
    #text = ' '.join([ps.stem(word) for word in text.split()]) #stem
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # stopwords
    return text
    
dataset['text'] = dataset['text'].apply(clean_text)
dataset.head()
```

## TextBlob

- Calculate the expected polarity for all the tweets.
- Using a cut off score of 0, change the polarity numbers to positive and negative categories.
- Display the performance metrics of using Textblob on this dataset. 

```{python}
##python chunk
tweets = np.array(dataset['text'])
sentiments = np.array(dataset['airline_sentiment'])

from sklearn.model_selection import train_test_split

train_tweets, test_tweets, train_sentiments, test_sentiments = train_test_split(tweets, sentiments, test_size=0.20, random_state = 42)
train_tweets.shape
test_tweets.shape

#calculate sentiment for smaller example set
sentiment_polarity = [textblob.TextBlob(tweet).sentiment.polarity for tweet in test_tweets]

#convert to categorical labels
predicted_sentiments = ['positive' if score >= 0 else 'negative' for score in sentiment_polarity]

#display performance metrics
display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, 
                                  classes=['positive', 'negative'])
```

## AFINN

- Calculate the expected polarity for all the tweets using AFINN.
- Using a cut off score of 0, change the polarity numbers to positive and negative categories.
- Display the performance metrics of using AFINN on this dataset. 

```{python}
##python chunk
from afinn import Afinn

#load the model 
afn = Afinn(emoticons=True)

#predict the polarity
sentiment_polarity = [afn.score(tweet) for tweet in test_tweets]

#decide how to categorize
predicted_sentiments = ['positive' if score >= 0 else 'negative' for score in sentiment_polarity]

#display performance metrics
display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, 
                                  classes=['positive', 'negative'])
```

## Split the dataset

- Split the dataset into training and testing datasets. 

```{python}
##python chunk
train_tweets, test_tweets, train_sentiments, test_sentiments = train_test_split(tweets, sentiments, test_size=0.20, random_state = 42)
train_tweets.shape
test_tweets.shape
```

## TF-IDF

- Calculate features for testing and training using the TF-IDF vectorizer.

```{python}
##python chunk
from sklearn.feature_extraction.text import TfidfVectorizer

# build TFIDF features on train tweets
tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),
                     sublinear_tf=True)
tv_train_features = tv.fit_transform(train_tweets)
tv_test_features = tv.transform(test_tweets)
```

## Logistic Regression Classifier

- Create a blank logistic regression model.
- Fit the the model to the training data.
- Create the predicted value for the testing data.

```{python}
##python chunk
from sklearn.linear_model import LogisticRegression

#blank model
lr = LogisticRegression(penalty='l2', max_iter=1000, C=1)

# fit the model
lr_tfidf_model = lr.fit(tv_train_features, train_sentiments)

# grab the predictions
lr_tfidf_predictions = lr_tfidf_model.predict(tv_test_features)
```

## Accuracy and Classification Report

- Display the performance metrics of the logistic regression model on the testing data.

```{python}
##python chunk
#model performance
display_model_performance_metrics(true_labels=test_sentiments,
  predicted_labels=lr_tfidf_predictions,
  classes=['positive', 'negative'])
```

## Topic Model Positive Reviews

- Create a dataset of just the positive reviews. 
- Create a dictionary and document term matrix to start the topics model.

```{python}
##python chunk
#load packages
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
import gensim
import gensim.corpora as corpora

#create separate datasets of positive and negative tweets 
positive = dataset[dataset['airline_sentiment']=="positive"]
positive_tweets = positive['text'].apply(nltk.word_tokenize)

negative = dataset[dataset['airline_sentiment']=="negative"]
negative_tweets = negative['text'].apply(nltk.word_tokenize)

#create a dictionary of the words
dictionary_positive = corpora.Dictionary(positive_tweets)
dictionary_negative = corpora.Dictionary(negative_tweets)

#create a doc term matrix
pos_doc_term_matrix = [dictionary_positive.doc2bow(doc) for doc in positive_tweets]
neg_doc_term_matrix = [dictionary_negative.doc2bow(doc) for doc in negative_tweets]
```

## Topic Model

- Create the LDA Topic Model for the positive reviews with three topics.

```{python}
##python chunk
#create LDA topic model for +ve dictionary
lda_model_pos = gensim.models.ldamodel.LdaModel(
  corpus = pos_doc_term_matrix, #TDM
  id2word = dictionary_positive, #Dictionary
  num_topics = 10, 
  random_state = 100,
  update_every = 1,
  chunksize = 100,
  passes = 10,
  alpha = 'auto',
  per_word_topics = True)
```

- LDA Topic Model for the negative reviews with 10 topics.

```{python}
##python chunk
#create LDA topic model for -ve dictionary
lda_model_neg = gensim.models.ldamodel.LdaModel(
  corpus = neg_doc_term_matrix, #TDM
  id2word = dictionary_negative, #Dictionary
  num_topics = 10, 
  random_state = 100,
  update_every = 1,
  chunksize = 100,
  passes = 10,
  alpha = 'auto',
  per_word_topics = True)
```

## Terms for the Topics

- Print out the top terms for each of the positive topics. 

```{python}
##python chunk
#Top terms in +ve dictionary
print(lda_model_pos.print_topics())
```

- Top terms for each of the negative topics

```{python}
##python chunk
#Top terms in +ve dictionary
print(lda_model_neg.print_topics())
```


## Interpretation

- Which model best represented the polarity in the dataset? 
- Looking at the topics analysis, what are main positive components to the data?

Using the TF-IDF vectorizer to calculate train and test features and fitting a logistic regression model to the training data performed best for representing the polarity in the dataset. This method was able to attain an almost identical measure of accuracy, precision, recall and f1 score, 71.88%, 71.9%, 71.88% and 71.88% respectively. This is an improvement of 8% in accuracy from the next best performing model, AFINN.

The main positive components include:

- !
- @
- .
- ,
- love
- ?
- not
- ...
- I
- know
- 's
- ``
- ''
- )
- (
- really
- good
- but
- :
- best
- way
- haha
- us
- watching
- let
- http
- awesome
- now
- yes
- tonight